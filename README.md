<!-- Portada -->
<p align="center">
  <p align="center">
    <img src="img/portadaA4.png"         alt="Portada A4"            width="1200"/>
  </p>
</p>

<!-- Badges -->
<p align="center">
  <a href="https://github.com/mamanirafa/Testing/actions">
    <img src="https://img.shields.io/github/actions/workflow/status/mamanirafa/Testing/ci.yml" alt="Build Status"/>
  </a>
  <a href="LICENSE">
    <img src="https://img.shields.io/github/license/mamanirafa/Testing" alt="License"/>
  </a>
</p>

---
---

**Carrera:** Tecnicatura Superior en Ciencia de Datos e Inteligencia Artificial  
**Instituci√≥n:** Polit√©cnico Malvinas Argentinas  
**Materia:** Aprendizaje Autom√°tico

**Proyecto:** Predicci√≥n del volumen sostenible de captura de Centolla en Tierra del Fuego

- **Alumno:** MAMANI, Rafael
- **Profesor:** Mart√≠n Mirabete

---

![√çndice futurista](img/indice.png)

### Accesos r√°pidos:
- [Resumen](#resumen)
- [Objetivo General](#objetivo-general)
- [Objetivos Espec√≠ficos](#objetivos-espec√≠ficos)
- [Contexto y Relevancia](#contexto-y-relevancia)
- [Origen y Descripci√≥n de los Datos](#origen-y-descripci√≥n-de-los-datos)
- [Estructura del Repositorio](#estructura-del-repositorio)
- [Licencia](#licencia)
---
Predicci√≥n de Producci√≥n Sostenible de Trucha en TDF mediante Modelos de Aprendizaje Autom√°tico
==============================
# Descripci√≥n del Proyecto
## üîç Resumen

Este proyecto de aprendizaje autom√°tico busca predecir el volumen sostenible de capturas de centolla (Lithodes
santolla) por a√±o en Tierra del Fuego, utilizando un modelo de regresi√≥n implementado en Python con scikitlearn. El dataset principal, ‚ÄúPesca y Puertos Pesqueros‚Äù del Ministerio de Econom√≠a, contiene datos hist√≥ricos
que permiten identificar patrones para la predicci√≥n. El proyecto utiliza la plantilla Cookiecutter Data Science y
un repositorio Git para garantizar reproducibilidad, con resultados documentados. El
modelo no realiza un an√°lisis estad√≠stico, sino que aprende patrones de los datos para generar predicciones
√∫tiles para simulaciones y toma de decisiones en la gesti√≥n pesquera.

---

## üé¨ Objetivo General
Desarrollar un modelo de regresi√≥n que prediga el volumen sostenible de capturas de centolla (en toneladas)
por a√±o en Tierra del Fuego, basado en variables como esfuerzo pesquero, condiciones ambientales y
regulaciones, para apoyar la gesti√≥n pesquera sostenible.

![Ejemplo de uso](img/objetivofin.png)

---

## ‚öôÔ∏è Objetivos Espec√≠ficos
1. Preprocesar el dataset para filtrar datos de Tierra del Fuego y centolla, asegurando patrones claros para
el aprendizaje del modelo.
2. Implementar un modelo de regresi√≥n (Random Forest Regressor o Linear Regression) para predecir el
volumen sostenible.
3. Modelo con m√©tricas como MSE, MAE y R¬≤, validando su capacidad para aprender patrones predictivos.
4. Generar simulaciones de vol√∫menes sostenibles para recomendar cuotas de pesca optimizadas.

---

## üöÄ Contexto y Relevancia
Tierra del Fuego,depende de la pesca de centolla como pilar econ√≥mico en puertos como Ushuaia y R√≠o Grande.
La sobreexplotaci√≥n y el cambio clim√°tico amenazan este recurso, mientras que el aislamiento geogr√°fico
complica la log√≠stica. La certificaci√≥n MSC de la pesquer√≠a de centolla resalta la importancia de la sostenibilidad
por lo que predecir vol√∫menes sostenibles es relevante para: (1) proteger la biodiversidad marina, (2) sostener
la econom√≠a local, (3) informar cuotas de pesca, y (4) adaptarse a cambios ambientales. La originalidad radica
en aplicar regresi√≥n a datos espec√≠ficos de centolla en una regi√≥n remota, generando simulaciones pr√°cticas.

---

## üõ† Origen y Descripci√≥n de los datos

### Origen del Dataset

*   **Instituto Nacional de Estad√≠sticas (INE) Chile**, Global Fishing Watch, y fuentes oficiales de regulaci√≥n pesquera.
*   **Fecha de Adquisici√≥n:** Junio de 2025.
*   **Recopilaci√≥n:** Datos recolectados y consolidados a partir de informes oficiales de desembarques, registros satelitales de esfuerzo pesquero, datos oceanogr√°ficos y normativas vigentes.
*   **Pol√≠tica de Datos:** Informaci√≥n p√∫blica y gratuita, sujeta a procesos de validaci√≥n y depuraci√≥n para garantizar calidad y consistencia.

### Descripci√≥n del Dataset

*   **Cantidad de Instancias:** Datos mensuales correspondientes a capturas, esfuerzo pesquero, temperatura superficial y regulaci√≥n de veda.
*   **Per√≠odo Cubierto:** Desde enero de 2019 hasta diciembre de 2024, totalizando 72 meses.
*   **Frecuencia:** Datos agrupados a nivel mensual por puerto (principalmente Punta Arenas).
*   **Total de Registros:** Aproximadamente 784 instancias, integrando m√∫ltiples variables para an√°lisis y modelado.

### Caracter√≠sticas (Columnas) y Tipos de Datos

*   **A√±o:** A√±o de registro (tipo: entero).
*   **Mes:** Mes de registro (tipo: entero o texto abreviado).
*   **Especie:** Nombre com√∫n o cient√≠fico de la especie (tipo: texto).
*   **Volumen de Captura (toneladas):** Cantidad de centolla desembarcada (tipo: float).
*   **Esfuerzo Pesquero (horas):** Horas de pesca reportadas en la zona (tipo: float).
*   **N√∫mero de Embarcaciones:** Cantidad de embarcaciones activas en la pesca (tipo: entero).
*   **Tipo de Arte:** M√©todo o equipo de pesca utilizado (tipo: texto).
*   **Temperatura Superficial del Mar (¬∞C):** Temperatura promedio mensual registrada en la zona de pesca (tipo: float).
*   **Estado de Veda:** Indicador binario o categ√≥rico que se√±ala si hay veda vigente en el mes (tipo: booleano o texto).
*   **Variables adicionales:** Variables calculadas o derivadas para el an√°lisis (tipo: varios).

### Informaci√≥n Relevante Adicional

*   **Datos Preliminares:** Los datos se encuentran sujetos a posibles actualizaciones y correcciones posteriores a procesos de limpieza y validaci√≥n.
*   **Valores Faltantes:** Existen registros con datos ausentes o nulos en algunas variables, particularmente en el esfuerzo pesquero y temperatura, que fueron tratados durante la limpieza.
*   **Consistencia:** Se aplicaron procesos de depuraci√≥n para homogeneizar nombres de especies, formatos de fecha y codificaci√≥n de variables categ√≥ricas.

---
---

## An√°lisis exploratorio de datos

Se realizaron an√°lisis gr√°ficos y estad√≠sticos, incluyendo:

- Captura total anual y promedio mensual de centolla.  
- Relaci√≥n entre esfuerzo pesquero, temperatura y volumen capturado.  
- Impacto de la veda en los vol√∫menes de pesca.

![graficos](img/grafico.png)

---

## Conclusiones del an√°lisis exploratorio

- La captura presenta marcada estacionalidad con meses pico.  
- La veda reduce el volumen capturado, evidenciando la efectividad regulatoria.  
- El esfuerzo pesquero y la temperatura superficial influyen en la cantidad capturada, mostrando correlaciones a considerar en el modelado.

---

## Modelo de Aprendizaje Autom√°tico

- Algoritmos utilizados: Regresi√≥n Lineal y Random Forest Regressor.  
- Variables predictoras: Esfuerzo pesquero, temperatura, indicadores de veda, y datos temporales.  
- Se aplicaron t√©cnicas de limpieza, normalizaci√≥n y validaci√≥n cruzada para garantizar robustez.

---

## Evaluaci√≥n del modelo

| M√©trica               | Regresi√≥n Lineal | Random Forest |
|-----------------------|------------------|---------------|
| MAE (Error absoluto)   | 7.90             | 0.14          |
| MSE (Error cuadr√°tico) | 102.44           | 0.18          |
| R¬≤ (Determinaci√≥n)     | 0.11             | 1.00          |

*(Incluir gr√°ficos de predicci√≥n vs real para ambos modelos)*

---

## Interpretaci√≥n y conclusiones finales

- El modelo Random Forest mostr√≥ un desempe√±o superior, capturando con precisi√≥n la variabilidad en los datos.  
- El modelo puede servir como herramienta predictiva para apoyar la toma de decisiones en la gesti√≥n pesquera.  
- Limitaciones incluyen la cantidad y calidad de datos, as√≠ como posibles variables no consideradas.

---

## Recomendaciones

- Ampliar el dataset con m√°s variables ambientales y biol√≥gicas.  
- Probar otros modelos y t√©cnicas avanzadas de machine learning.  
- Implementar monitoreo continuo para actualizar el modelo peri√≥dicamente.

---

## Entregables en el repositorio

- Notebooks: Exploraci√≥n, an√°lisis, modelado y depuraci√≥n.  
- Dataset procesado (`dataset_modelado_final.csv`).  
- Gr√°ficos y reportes generados.  
- Documentos descriptivos y presentaci√≥n ejecutiva.

---

---
Project Organization
------------

    ‚îú‚îÄ‚îÄ LICENSE
    ‚îú‚îÄ‚îÄ Makefile           <- Makefile with commands like `make data` or `make train`
    ‚îú‚îÄ‚îÄ README.md          <- The top-level README for developers using this project.
    ‚îú‚îÄ‚îÄ data
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ external       <- Data from third party sources.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ interim        <- Intermediate data that has been transformed.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ processed      <- The final, canonical data sets for modeling.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ raw            <- The original, immutable data dump.
    ‚îÇ
    ‚îú‚îÄ‚îÄ docs               <- A default Sphinx project; see sphinx-doc.org for details
    ‚îÇ
    ‚îú‚îÄ‚îÄ models             <- Trained and serialized models, model predictions, or model summaries
    ‚îÇ
    ‚îú‚îÄ‚îÄ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    ‚îÇ                         the creator's initials, and a short `-` delimited description, e.g.
    ‚îÇ                         `1.0-jqp-initial-data-exploration`.
    ‚îÇ
    ‚îú‚îÄ‚îÄ references         <- Data dictionaries, manuals, and all other explanatory materials.
    ‚îÇ
    ‚îú‚îÄ‚îÄ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ figures        <- Generated graphics and figures to be used in reporting
    ‚îÇ
    ‚îú‚îÄ‚îÄ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    ‚îÇ                         generated with `pip freeze > requirements.txt`
    ‚îÇ
    ‚îú‚îÄ‚îÄ setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    ‚îú‚îÄ‚îÄ src                <- Source code for use in this project.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py    <- Makes src a Python module
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data           <- Scripts to download or generate data
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ make_dataset.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ features       <- Scripts to turn raw data into features for modeling
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ build_features.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models         <- Scripts to train models and then use trained models to make
    ‚îÇ   ‚îÇ   ‚îÇ                 predictions
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ predict_model.py
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ train_model.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ visualization  <- Scripts to create exploratory and results oriented visualizations
    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ visualize.py
    ‚îÇ
    ‚îî‚îÄ‚îÄ tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io


--------